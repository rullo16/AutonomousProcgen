{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "from agents.DistillPPO import DistillPPOAgent\n",
    "from common.hyperparameters import HYPERPARAMS\n",
    "import torch\n",
    "from common.env import make_env\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.login()\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset of procgen Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## heist: \n",
    " The player must steal the gem hidden behind a network of locks. Each lock comes in one of three colors, and the necessary keys to open these locks are scattered throughout the level. The level layout takes the form of a maze, again generated by Kruskal's algorithm. Once the player collects a key of a certain color, the player may open the lock of that color. All keys in the player's possession are shown in the top right corner of the screen.\n",
    "- ## Bossfight\n",
    "The player controls a small starship and must destroy a much bigger boss starship. The boss randomly selects from a set of possible attacks when engaging the player. The player must dodge the incoming projectiles or be destroyed. The player can also use randomly scattered meteors for cover. After a set timeout, the boss becomes vulnerable and its shields go down. At this point, the players projectile attacks will damage the boss. Once the boss receives a certain amount of damage, the player receives a reward, and the boss re-raises its shields. If the player damages the boss several times in this way, the boss is destroyed, the player receives a large reward, and the episode ends.\n",
    "- ## leaper\n",
    "Inspired by the classic game “Frogger”. The player must cross several lanes to reach the finish line and earn a reward. The first group of lanes contains cars which must be avoided. The second group of lanes contains logs on a river. The player must hop from log to log to cross the river. If the player falls in the river, the episode ends.\n",
    "\n",
    "descriptions from [OpenAI](https://openai.com/index/procgen-benchmark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilled PPO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = HYPERPARAMS['distill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'heist'\n",
    "env = make_env(game, n_envs=params.num_envs) #Initialise just for the agent to get obs and action spaces\n",
    "log = wandb.init(project=f\"procgen-ppo-explorations-{game}-{params.name}\", config=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DistillPPOAgent(env.observation_space.shape, env.action_space.n, params=params)\n",
    "env = make_env(game, n_envs=params.num_envs, params=params, student_model=agent.student, teacher_model= agent.teacher)\n",
    "test_reward = []\n",
    "best_reward = None\n",
    "mean_rewards = []\n",
    "steps = 0\n",
    "test_count = 0\n",
    "\n",
    "while steps < params.total_epochs:\n",
    "    tot_reward_episode = []\n",
    "    state = env.reset()  \n",
    "    done = np.zeros(params.num_envs)\n",
    "    for _ in range(params.traj_steps):\n",
    "        action, log_prob, value_ext, value_int = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.trajectories.log_transition(state, action, reward, done, log_prob, value_ext, value_int)\n",
    "        state = next_state\n",
    "        tot_reward_episode.append(reward[0])\n",
    "    _,_,last_ext_value, last_int_value = agent.select_action(state)\n",
    "    agent.trajectories.save_final_state(state, last_ext_value, last_int_value)\n",
    "    mean_reward = np.mean(np.sum([tot_reward_episode[i][0] for i in range(len(tot_reward_episode))]))\n",
    "    tot_reward_episode = []\n",
    " \n",
    "    '''\n",
    "    Log mean reward of only one environment\n",
    "    '''\n",
    "    wandb.log({\"mean_reward\": mean_reward})\n",
    "    print(f\"Episode: {steps}\")\n",
    "    mean_rewards.append(mean_reward)\n",
    "\n",
    "    ''' \n",
    "    Compute estimates of advantage function and the discounted returns\n",
    "    Try to decrease gamma as training goes on\n",
    "    '''\n",
    "    agent.trajectories.calculate_advantages_and_returns(discount_factor=min(0.999, params.gamma + steps * (params.total_epochs - params.gamma) / params.total_epochs*0.1))\n",
    "    agent.trajectories.compute_reference_values(discount_factor=min(0.999, params.gamma + steps * (params.total_epochs - params.gamma) / params.total_epochs*0.1))\n",
    "\n",
    "    print(\"TRAINING!!!!\")\n",
    "    agent.train(steps)\n",
    "    steps += params.traj_steps * params.num_envs\n",
    "    agent.optimizer = agent.improv_lr(agent.optimizer,params.lr, steps, params.total_epochs)\n",
    "    agent.distillation_optimizer = agent.improv_lr(agent.distillation_optimizer,params.lr_distillation, steps, params.total_epochs)\n",
    "    if steps > ((test_count+1) * (params.total_epochs // params.tests_to_do)):\n",
    "        print(\"TESTING!!!!!!!!!!\")\n",
    "        agent.net.train(False)\n",
    "        ts = time.time()\n",
    "        rewards = agent.testing(game)\n",
    "        print(f\"Test finished in {time.time() - ts:.2f}s, Test Rewards {rewards}\")\n",
    "        test_reward.append(rewards)\n",
    "        wandb.log({\"test_reward\": rewards})\n",
    "        if best_reward is None or best_reward < rewards:\n",
    "            print(f\"New Best Reward: {rewards}\")\n",
    "            best_reward = rewards\n",
    "            name = f\"checkpoints/best_{params.name}_{game}.dat\"\n",
    "            torch.save(agent.net.state_dict(), name)\n",
    "            if best_reward >= params.stop_reward:\n",
    "                print(f\"Stopped Training!\")\n",
    "                break\n",
    "        agent.net.train(True)\n",
    "        test_count += 1\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
