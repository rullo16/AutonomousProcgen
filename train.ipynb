{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrullofederico16\u001b[0m (\u001b[33mfede-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "from agents.DistillPPO import DistillPPOAgent\n",
    "from common.hyperparameters import HYPERPARAMS\n",
    "import torch\n",
    "from common.env import make_env\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.login()\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset of procgen Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## heist: \n",
    " The player must steal the gem hidden behind a network of locks. Each lock comes in one of three colors, and the necessary keys to open these locks are scattered throughout the level. The level layout takes the form of a maze, again generated by Kruskal's algorithm. Once the player collects a key of a certain color, the player may open the lock of that color. All keys in the player's possession are shown in the top right corner of the screen.\n",
    "- ## Bossfight\n",
    "The player controls a small starship and must destroy a much bigger boss starship. The boss randomly selects from a set of possible attacks when engaging the player. The player must dodge the incoming projectiles or be destroyed. The player can also use randomly scattered meteors for cover. After a set timeout, the boss becomes vulnerable and its shields go down. At this point, the players projectile attacks will damage the boss. Once the boss receives a certain amount of damage, the player receives a reward, and the boss re-raises its shields. If the player damages the boss several times in this way, the boss is destroyed, the player receives a large reward, and the episode ends.\n",
    "- ## leaper\n",
    "Inspired by the classic game “Frogger”. The player must cross several lanes to reach the finish line and earn a reward. The first group of lanes contains cars which must be avoided. The second group of lanes contains logs on a river. The player must hop from log to log to cross the river. If the player falls in the river, the episode ends.\n",
    "\n",
    "descriptions from [OpenAI](https://openai.com/index/procgen-benchmark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilled PPO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = HYPERPARAMS['distill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fede/AutonomousProcgen/wandb/run-20240903_194500-51xacen7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill/runs/51xacen7' target=\"_blank\">youthful-cosmos-10</a></strong> to <a href='https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill' target=\"_blank\">https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill/runs/51xacen7' target=\"_blank\">https://wandb.ai/fede-/procgen-ppo-explorations-leaper-distill/runs/51xacen7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = 'leaper'\n",
    "env = make_env(game, n_envs=params.num_envs) #Initialise just for the agent to get obs and action spaces\n",
    "log = wandb.init(project=f\"procgen-ppo-explorations-{game}-{params.name}\", config=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 6144, Mean Reward Episode of the first env: 10.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 12288, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 18432, Mean Reward Episode of the first env: 10.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 24576, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 30720, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 36864, Mean Reward Episode of the first env: 10.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 43008, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 49152, Mean Reward Episode of the first env: 10.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 55296, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 61440, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 67584, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 73728, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 79872, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 86016, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 92160, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 98304, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 104448, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 110592, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 116736, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 122880, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 129024, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 135168, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 141312, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 147456, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 153600, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 159744, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 165888, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 172032, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 178176, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 184320, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 190464, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "Warning: manual reset ignored\n",
      "Episode: 196608, Mean Reward Episode of the first env: 0.00\n",
      "Training\n",
      "TESTING!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/miniconda3/envs/procgen/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/fede/miniconda3/envs/procgen/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/fede/miniconda3/envs/procgen/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/fede/miniconda3/envs/procgen/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/fede/miniconda3/envs/procgen/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "agent = DistillPPOAgent(env.observation_space.shape, env.action_space.n, params=params)\n",
    "env = make_env(game, n_envs=params.num_envs, params=params, student_model=agent.student, teacher_model= agent.teacher)\n",
    "test_reward = []\n",
    "test_steps = []\n",
    "best_reward = None\n",
    "mean_rewards = []\n",
    "steps = 0\n",
    "test_count = 0\n",
    "\n",
    "while steps < params.total_epochs:\n",
    "    tot_reward_episode = []\n",
    "    state = env.reset()  \n",
    "    done = np.zeros(params.num_envs)\n",
    "    for _ in range(params.traj_steps):\n",
    "        action, log_prob, value_ext, value_int = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.trajectories.log_transition(state, action, reward, done, log_prob, value_ext, value_int)\n",
    "        state = next_state\n",
    "        tot_reward_episode.append(reward[0])\n",
    "    _,_,last_ext_value, last_int_value = agent.select_action(state)\n",
    "    agent.trajectories.save_final_state(state, last_ext_value, last_int_value)\n",
    "    mean_reward = np.mean(np.sum([tot_reward_episode[i][0] for i in range(len(tot_reward_episode))]))\n",
    "    tot_reward_episode = []\n",
    "\n",
    "    '''\n",
    "    Log mean reward of only one environment\n",
    "    '''\n",
    "    wandb.log({\"mean_reward\": mean_reward})\n",
    "    print(f\"Episode: {steps}\")\n",
    "    mean_rewards.append(mean_reward)\n",
    "\n",
    "    ''' \n",
    "    Compute estimates of advantage function and the discounted returns\n",
    "    Try to decrease gamma as training goes on\n",
    "    '''\n",
    "    agent.trajectories.calculate_advantages_and_returns(discount_factor=min(0.999, params.gamma + steps * (params.total_epochs - params.gamma) / params.total_epochs*0.1))\n",
    "    agent.trajectories.compute_reference_values(discount_factor=min(0.999, params.gamma + steps * (params.total_epochs - params.gamma) / params.total_epochs*0.1))\n",
    "\n",
    "    print(\"TRAINING!!!!\")\n",
    "    agent.train(steps)\n",
    "    steps += params.traj_steps * params.num_envs\n",
    "    agent.optimizer = agent.improv_lr(agent.optimizer,params.lr, steps, params.total_epochs)\n",
    "    agent.distillation_optimizer = agent.improv_lr(agent.distillation_optimizer,params.lr_distillation, steps, params.total_epochs)\n",
    "    if steps > ((test_count+1) * (params.total_epochs // params.tests_to_do)):\n",
    "        print(\"TESTING!!!!!!!!!!\")\n",
    "        agent.net.train(False)\n",
    "        ts = time.time()\n",
    "        rewards, test_step = agent.testing(game)\n",
    "        print(f\"Test finished in {time.time() - ts:.2f}s, Test Rewards {rewards}\")\n",
    "        test_reward.append(rewards)\n",
    "        test_steps.append(test_step)\n",
    "        wandb.log({\"test_reward\": rewards, \"test_steps\": test_step})\n",
    "        if best_reward is None or best_reward < rewards:\n",
    "            print(f\"New Best Reward: {rewards}\")\n",
    "            best_reward = rewards\n",
    "            name = f\"checkpoints/best_{params.name}_{game}.dat\"\n",
    "            torch.save(agent.net.state_dict(), name)\n",
    "            if best_reward >= params.stop_reward:\n",
    "                print(f\"Stopped Training!\")\n",
    "                break\n",
    "        agent.net.train(True)\n",
    "        test_count += 1\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
